import calendar
import pandas as pd
import numpy as np
import logging



#preprocesamiento noviembre 5 2020
def preprocesamiento(dframe, prediccion_corta = True):
    
    logger = logging.getLogger('LogdemiApp')


    logger.debug('pasamos ds a datetime')
    dframe['ds'] = pd.to_datetime(dframe['ds'], format='%Y-%m-%d')
    
    logger.debug('corremos funcion week of the month y guardamos resultado en week_nr')
    dframe['week_nr'] = dframe['ds'].apply(week_of_month)

    
    
    
    #usamos branch_lag como un regresor ejemplo para parametrizar el subset del dataframe para la prediccion
    #nos quedamos con el dataframe donde un regresor que vamos a usar para predecir en su version
    #lag no tiene NA values, el resto no nos sirve.
    
    if prediccion_corta == True:
        dframe = dframe[dframe['branch_lag'].notna()]
        logger.debug('prediccion corta -> dropeamos las rows que tengan NA en branch lag')
    
    
    #NO SE USA BORRAR MAS ADELANTE DIAS PREDICT
    #tambien guardaremos en el dataframe una columna con los dias de prediccion (se puede usar un diccionario)
    
    
    #dropeamos canal
    dframe.drop(columns='canal', inplace=True)
    
    
    #generamos la variable posfe_habil, que es 1 unicamente cuando el dia  es habil y posferiado a la vez.
    dframe.loc[(dframe['posferiado'] ==1) & (dframe['habil'] ==1), 'posfe_habil'] = 1
    dframe.loc[(dframe['posfe_habil'].isna()), 'posfe_habil'] = 0
    
    dframe_access = dframe.rename(columns={'y_access':'y'}).drop(columns='y_mobile')
    dframe_access['dias_predict'] = (dframe_access.branch_lag.notna().sum() - dframe_access.y.notna().sum()) 
    
    
    dframe_mobile = dframe.rename(columns={'y_mobile':'y'}).drop(columns='y_access')
    dframe_mobile['dias_predict'] = (dframe_mobile.branch_lag.notna().sum() - dframe_mobile.y.notna().sum()) 
    
    
    return dframe_mobile, dframe_access




#funcion que pone el numero de la semana del mes, del 1 al 5

def week_of_month(date):
    logger = logging.getLogger('LogdemiApp')
    logger.debug('week of month ready')
    """Determines the week (number) of the month"""

    #Calendar object. 6 = Start on Sunday, 0 = Start on Monday
    cal_object = calendar.Calendar(6)
    month_calendar_dates = cal_object.itermonthdates(date.year,date.month)

    day_of_week = 1
    week_number = 1

    for day in month_calendar_dates:
        #add a week and reset day of week
        if day_of_week > 7:
            week_number += 1
            day_of_week = 1

        if date == day:
            break
        else:
            day_of_week += 1

    logger.debug('return')

    return week_number


def randomforest_FE(X_train,Y_train,featurecols, n_iter=5, cv=3, scoring='explained_variance'):
    from sklearn.model_selection import RandomizedSearchCV
    from sklearn.ensemble import RandomForestRegressor
    import seaborn as sns
    import matplotlib.pyplot as plt
	    
	    
    
    #grid de hiperparametros
    # Number of trees in random forest
    n_estimators = [int(x) for x in np.linspace(start = 20, stop = 150, num = 5)]
    # Number of features to consider at every split
    max_features = ['auto', 'sqrt']
    # Maximum number of levels in tree
    max_depth = [int(x) for x in np.linspace(5, 200, num = 5)]
    max_depth.append(None)
    # Minimum number of samples required to split a node
    min_samples_split = [2, 5, 10]
    # Minimum number of samples required at each leaf node
    min_samples_leaf = [1, 2, 4]
    # Method of selecting samples for training each tree
    bootstrap = [True, False]
    # Create the random grid
    random_grid = {'n_estimators': n_estimators,
                   'max_features': max_features,
                   'max_depth': max_depth,
                   'min_samples_split': min_samples_split,
                   'min_samples_leaf': min_samples_leaf,
                   'bootstrap': bootstrap}    
    
    rf = RandomForestRegressor()
    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = n_iter, cv = cv, verbose=1,
                                   n_jobs = -1, scoring='explained_variance')
    rf_random.fit(X_train,Y_train)
    best_params = rf_random.best_params_
    #usamos el mejor modelo y lo fiteamos para sacar el feat importance
    rf.set_params(**best_params)
    rf.fit(X_train, Y_train)
    
    #ordenamos por feat importance, ordenamos y ploteamos
    sorted_idx = rf.feature_importances_.argsort()
    dict_featimp = {'variables':X_train.columns[sorted_idx] , 'feat_imp':rf.feature_importances_[sorted_idx]}
    feat_importances = pd.DataFrame(data=dict_featimp).sort_values(by='feat_imp', ascending=False)
    
    plt.figure(figsize=(7,6))
    b = sns.barplot(data=feat_importances[feat_importances.feat_imp > 0.03], x='feat_imp', y='variables',ci=None)
    
    
    _, ylabels = plt.yticks()
    b.set_yticklabels(ylabels, size=15)
    

    plt.show()
    

    print(feat_importances[feat_importances.feat_imp > 0.01])
    
    
