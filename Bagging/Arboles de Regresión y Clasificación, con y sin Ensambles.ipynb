{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arboles de clasificación y regresión simples vs bagging (ensambles)\n",
    "\n",
    "En este lab vamos a comparar el rendimiento de un caso de árbol de **clasificación** y otro de **regresión** con el rendimiento de cada versión implementando **bagging**.\n",
    "\n",
    "El método de bagging busca generar un conjunto de modelos que en promedio predigan mejor, utilizando técnicas de resampling (bootstrapping) sobre el set de muestras de entrenamiento.\n",
    "\n",
    "En el caso de la regresión, tendremos un promedio de los outputs de todos los modelos. En el caso de la clasificación, el voto de la mayoría determinará el resultado binario en este caso. \n",
    "\n",
    "La ventaja es la reducción de la varianza o overfitting y corrigen así este problema que tienen los árboles en general logrando una combinación exitosa.\n",
    "\n",
    "Usaremos dos datasets:\n",
    "\n",
    "Clasificación, utilizaremos el dataset cancer de mama de sklearn.\n",
    "\n",
    "Regresión, utilizaremos el dataset diabetes también de sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Levantamos el dataset breast cancer de sklearn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "y = data['target']\n",
    "X = data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seteamos parámetros para CV: Folds y porcentaje train/test\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=41, shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort=False,\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=1.0, n_estimators=10, n_jobs=None,\n",
       "                  oob_score=False, random_state=None, verbose=0,\n",
       "                  warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instanciamos y entrenamos un arbol de clasificación y un arbol de clasificación con bagging\n",
    "\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', random_state=1)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "bdt = BaggingClassifier(DecisionTreeClassifier())\n",
    "bdt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy dt= 0.9415204678362573\n",
      "[[ 57   4]\n",
      " [  6 104]]\n",
      "\n",
      "Accuracy bdt= 0.9766081871345029\n",
      "[[ 60   1]\n",
      " [  3 107]]\n"
     ]
    }
   ],
   "source": [
    "#probamos la performance de los árboles en el conjunto test con accuracy y matriz de confusión\n",
    "\n",
    "dt_y_pred = dt.predict(X_test)\n",
    "print('Accuracy dt=', accuracy_score(y_test, dt_y_pred))\n",
    "confusiondt = confusion_matrix(y_test, dt_y_pred)\n",
    "print(confusiondt)\n",
    "\n",
    "print ()\n",
    "\n",
    "bdt_y_pred = bdt.predict(X_test)\n",
    "print('Accuracy bdt=', accuracy_score(y_test, bdt_y_pred))\n",
    "confusionbdt = confusion_matrix(y_test, bdt_y_pred)\n",
    "print(confusionbdt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Vemos una mejora clara en el resultado del clasificador con bagging\n",
    "\n",
    "Ahora podemos probar un tipo de bagging específico llamado Random Forest. Toma subconjuntos aleatorios de features para cada caso. Para buscar hiperparámetros, vamos a utilizar Gridsearch y tratar de mejorar el resultado anterior!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciamos el clasificador y definimos el rango de iteración para el Gridsearch\n",
    "\n",
    "rf = RandomForestClassifier(class_weight='balanced', random_state=1)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_trees = {'n_estimators': [50, 100, 200], \n",
    "               'max_features': [1, 5, 8, 10, 21], \n",
    "               'max_depth': [5, 20, 50, 70, 100], \n",
    "               'min_samples_leaf':[1, 5, 8, 10, 50]}\n",
    "grid_search_rf = GridSearchCV(rf, param_grid=param_trees, cv=cv , verbose=1, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 375 candidates, totalling 1875 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1242 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1792 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1875 out of 1875 | elapsed:  5.8min finished\n",
      "C:\\Users\\Joaquin\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=41, shuffle=True),\n",
       "             error_score='raise-deprecating',\n",
       "             estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                              class_weight='balanced',\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators='warn', n_jobs=None,\n",
       "                                              oob_score=False, random_state=1,\n",
       "                                              verbose=0, warm_start=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid={'max_depth': [5, 20, 50, 70, 100],\n",
       "                         'max_features': [1, 5, 8, 10, 21],\n",
       "                         'min_samples_leaf': [1, 5, 8, 10, 50],\n",
       "                         'n_estimators': [50, 100, 200]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#entrenamos el modelo\n",
    "\n",
    "grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy grid_search_rf = 0.9941520467836257\n",
      "[[ 61   0]\n",
      " [  1 109]]\n"
     ]
    }
   ],
   "source": [
    "#Probamos el rendimiento en test\n",
    "\n",
    "grid_search_rf_y_pred = grid_search_rf.predict(X_test)\n",
    "print('Accuracy grid_search_rf =', accuracy_score(y_test, grid_search_rf_y_pred))\n",
    "confusion_grid_search_rf = confusion_matrix(y_test, grid_search_rf_y_pred)\n",
    "print(confusion_grid_search_rf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logramos una clasificación casi perfecta! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instalamos las librerías necesarias e importamos el set de datos Diabetes\n",
    "\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingRegressor(base_estimator=DecisionTreeRegressor(criterion='mse',\n",
       "                                                      max_depth=2,\n",
       "                                                      max_features=None,\n",
       "                                                      max_leaf_nodes=None,\n",
       "                                                      min_impurity_decrease=0.0,\n",
       "                                                      min_impurity_split=None,\n",
       "                                                      min_samples_leaf=1,\n",
       "                                                      min_samples_split=2,\n",
       "                                                      min_weight_fraction_leaf=0.0,\n",
       "                                                      presort=False,\n",
       "                                                      random_state=None,\n",
       "                                                      splitter='best'),\n",
       "                 bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                 max_samples=1.0, n_estimators=10, n_jobs=None, oob_score=False,\n",
       "                 random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instanciamos el arbol de decisión para regresión y su versión con bagging\n",
    "#Fiteamos ambos modelos\n",
    "\n",
    "dtr = DecisionTreeRegressor(max_depth=2)\n",
    "dtr.fit(X_train, y_train)\n",
    "\n",
    "bdtr = BaggingRegressor(dtr)\n",
    "bdtr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy dtr= 0.2604742200567519\n",
      "Accuracy bdtr= 0.3667665022817359\n"
     ]
    }
   ],
   "source": [
    "#Evaluamos el rendimiento en el conjunto TEST de los dos modelos\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "dtr_y_pred = dtr.predict(X_test)\n",
    "print('Accuracy dtr=', r2_score(y_test, dtr_y_pred))\n",
    "\n",
    "bdtr_y_pred = bdtr.predict(X_test)\n",
    "print('Accuracy bdtr=', r2_score(y_test, bdtr_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando la métrica R2 para comparar, vemos una mejora visible de la implementación con bagging!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
